{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "260832907_Assignment_1 (2021-09-18 22:08).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FuO82RTBftK"
      },
      "source": [
        "Go to https://drive.google.com/drive/folders/1Pe6D713L9S0GWwb_XzeLXAeNZOrBqZaN?usp=sharing and click add shortcut to drive. This will add the data required for this assignment to your Google drive.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LqHisiziX8Ri94Xs6Cv8mhx6vivFM3kS\" alt=\"Drawing\" height=\"300\"/>\n",
        "\n",
        "Caution: Since this is real language on Twitter and deals with world events, some of it could be disturbing. In the later section of the course, we will deal with ethics and what is appropriate and what is not. \n",
        "\n",
        "First, let's read the tweets from the Google drive and print a few lines from the file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZEcHthBeXz"
      },
      "source": [
        "Run the below code snippet. It will generate a URL which generates an authorization code. Enter it below to give Colab access to your Google drive. \n",
        "\n",
        "**Note:** Copy button may not work. Try copying the authorization code manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW-dce7oJlyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608f61a3-d98a-411e-9e6d-e6a5828de0f0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni2pYuuQKaHY"
      },
      "source": [
        "When you run the `ls` command below, you should see the files in the tweets folder.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYENtyc7SOxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5dc000f-0b1b-4a7c-acc0-29f27ea70fb3"
      },
      "source": [
        "!ls \"/content/drive/My Drive/tweets\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000_tweets.jsonl\n",
            "20000_tweets.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.tokenized.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.trigrams.txt\n",
            "GoogleNews-vectors-negative300.bin.gz\n",
            "stop_words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEvB8AMiqeNh"
      },
      "source": [
        "Let's read the top 10 tweets from the file `20000_tweets.txt` and print them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMOmElPSPHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e2fadd-3acc-4719-81ee-591b7fca733c"
      },
      "source": [
        "f = open(\"/content/drive/My Drive/tweets/20000_tweets.txt\", \"r\")\n",
        "\n",
        "line_count = 0\n",
        "top_tweets = []\n",
        "for tweet in f:\n",
        "  print(\"### Tweet\", line_count, \"#####\")\n",
        "  print(tweet)\n",
        "  \n",
        "  top_tweets.append(tweet)\n",
        "  line_count += 1\n",
        "  if line_count >= 10:\n",
        "    break\n",
        "f.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Tweet 0 #####\n",
            "Covid-19 Economic Response: Cancel Student Loans by Executive Order. - Sign the Petition! https://t.co/BnPXWHv5cr via @Change\n",
            "\n",
            "### Tweet 1 #####\n",
            "Hey! The stock markets up! Fuck Trump and the Trumpublican Senate! https://t.co/4t6mgbaG2C\n",
            "\n",
            "### Tweet 2 #####\n",
            "@ProjectLincoln blame Trump for no sports because of Covid19 @MeidasTouch\n",
            "\n",
            "### Tweet 3 #####\n",
            "Do u guys think that reopening of the dinning in the restaurants of Lahore will bring any change in our lives? Nothing is going to change I think cause many people have learned cooking, painting and many new stuff during lockdown and I think we all will continue those things 1/2\n",
            "\n",
            "### Tweet 4 #####\n",
            "@NBCSAthletics Ya just knew the season wouldn’t go by without some bench clearing brawls... Covid or not. Behaviors can’t be changed because of rules, sadly\n",
            "\n",
            "### Tweet 5 #####\n",
            "You’re comparing apples and oranges. The NCAA has a lot more to balance than the NFL. https://t.co/oeMsbgeem7\n",
            "\n",
            "### Tweet 6 #####\n",
            "THANK YOU @POTUS Absolutely NO MONEY/NO BAILOUTS for Irresponsibly Run Democrat Cities...#NoBailouts https://t.co/bWLELcCutV\n",
            "\n",
            "### Tweet 7 #####\n",
            "#Texas #USA #NorthAmerica Cases: 509,539 (+21) Death: 8,583 Recovered: 344,845 Critical: 1,754 New %: 0.3% Death %: 1.7% Population %: 1.8% #CoronaVirus #Covid19 #SarsCov2 #Forecast https://t.co/yHbd9gl1uz https://t.co/sRulRFOeUx\n",
            "\n",
            "### Tweet 8 #####\n",
            "New Zealand celebrated their 100th day without COVID today. A society that works together for the common good: that's what actual freedom looks like. https://t.co/7qppin3QRM\n",
            "\n",
            "### Tweet 9 #####\n",
            "It’s going to be screen time all the time for kindergartners and graduate students alike. Teachers are threatening strikes. And students are already coming home infected with the coronavirus, which has upended American education. https://t.co/O3eT06wmrU\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r-IHJ3KU27N"
      },
      "source": [
        "# **Problem 1: Word Tokenizer**\n",
        "\n",
        "Implement your own tokenizer for tweets without using libraries like `nltk`, `spacy` or any existing tokenizers. Tokenizer is a fundamental tool for processing any text data. The tokenizer should preserve punctuations.\n",
        "\n",
        "The outputs for the top five tweets should approximately be as follows (it's fine to not exactly match this output)\n",
        "\n",
        "```\n",
        "['Covid-19', 'Economic', 'Response', ':', 'Cancel', 'Student', 'Loans', 'by', 'Executive', 'Order', '.', '-', 'Sign', 'the', 'Petition', '!', 'https://t.co/BnPXWHv5cr', 'via', '@Change']\n",
        "['Hey', '!', 'The', 'stock', 'markets', 'up', '!', 'Fuck', 'Trump', 'and', 'the', 'Trumpublican', 'Senate', '!', 'https://t.co/4t6mgbaG2C']\n",
        "['@ProjectLincoln', 'blame', 'Trump', 'for', 'no', 'sports', 'because', 'of', 'Covid19', '@MeidasTouch']\n",
        "['Do', 'u', 'guys', 'think', 'that', 'reopening', 'of', 'the', 'dinning', 'in', 'the', 'restaurants', 'of', 'Lahore', 'will', 'bring', 'any', 'change', 'in', 'our', 'lives', '?', 'Nothing', 'is', 'going', 'to', 'change', 'I', 'think', 'cause', 'many', 'people', 'have', 'learned', 'cooking', ',', 'painting', 'and', 'many', 'new', 'stuff', 'during', 'lockdown', 'and', 'I', 'think', 'we', 'all', 'will', 'continue', 'those', 'things', '1/2']\n",
        "['@NBCSAthletics', 'Ya', 'just', 'knew', 'the', 'season', 'would', 'n’t', 'go', 'by', 'without', 'some', 'bench', 'clearing', 'brawls', '...', 'Covid', 'or', 'not', '.', 'Behaviors', 'ca', 'n’t', 'be', 'changed', 'because', 'of', 'rules', ',', 'sadly']\n",
        "```\n",
        "\n",
        "A thing to note is that words like `wouldn't` and `you'll` become two tokens `would n't` and `you 'll`, after tokenization. Treating `'nt` and  `'ll` as independent tokens is common in NLP tools. You can come up with that kind of list easily, e.g., `'s, 've`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y-oKzw7OHOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9512a3c5-4388-4ff8-aa91-9002d0e88a09"
      },
      "source": [
        "# import any relevant libraries here\n",
        "import string\n",
        "import re\n",
        "import copy\n",
        "\n",
        "common_contractions_2 = [\"'d\", \"'m\", \"'s\", \"’d\", \"’m\", \"’s\"]\n",
        "common_contractions_3 = [\"'ll\", \"n't\", \"'re\", \"'ve\", \"’ll\", \"n’t\", \"’re\", \"’ve\"]\n",
        "\n",
        "# special contractions\n",
        "sc_list = [\"cannot\", \"gimme\", \"gonna\", \"gotta\", \"'tis\", \"’tis\", \"'twas\", \"’twas\", \"wanna\", \"y'all\", \"y’all\"]\n",
        "sc_dic = {\"cannot\": [\"can\", \"not\"],\n",
        "          \"gimme\": [\"gim\", \"me\"],\n",
        "          \"gonna\": [\"gon\", \"na\"],\n",
        "          \"gotta\": [\"got\", \"ta\"],\n",
        "          \"'tis\": [\"'t\", \"is\"],\n",
        "          \"’tis\": [\"’t\", \"is\"],\n",
        "          \"'twas\": [\"'t\", \"was\"],\n",
        "          \"’twas\": [\"’t\", \"was\"],\n",
        "          \"wanna\": [\"wan\", \"na\"],\n",
        "          \"y'all\": [\"y\", \"'all\"],\n",
        "          \"y’all\": [\"y\", \"’all\"]\n",
        "          }\n",
        "\n",
        "# a function to tokenize text into words\n",
        "def tokenize(text):\n",
        "    # split text\n",
        "    # website OR number with .,/ OR hashtag OR username OR word with -'’ characters OR punctuation\n",
        "    words = re.findall(r\"https:[^\\s]+|[\\d][\\d\\.,/]+[\\d]+|[#][\\w]+|[@][\\w]+|[\\w\\-'’]+|[^\\s\\w\\-'’]\", text)\n",
        "\n",
        "    words_c = copy.deepcopy(words)\n",
        "\n",
        "    # handle contractions\n",
        "    for word in words:\n",
        "        # common contractions\n",
        "        # check if there is a ' or ’\n",
        "        apostrophe_1 = \"'\"\n",
        "        apostrophe_2 = \"’\"\n",
        "\n",
        "        if (apostrophe_1 in word) or (apostrophe_2 in word):\n",
        "\n",
        "            if (len(word) > 3) and (word[-3:] in common_contractions_3):\n",
        "                # split word\n",
        "                word_1 = word[:-3]\n",
        "                word_2 = word[-3:]\n",
        "\n",
        "                # replace contraction with two words\n",
        "                i = words_c.index(word)\n",
        "                words_c[i:i+1] = word_1, word_2\n",
        "\n",
        "            elif (len(word) > 2) and (word[-2:] in common_contractions_2):\n",
        "                # split word\n",
        "                word_1 = word[:-2]\n",
        "                word_2 = word[-2:]\n",
        "\n",
        "                # replace contraction with two words\n",
        "                i = words_c.index(word)\n",
        "                words_c[i:i+1] = word_1, word_2\n",
        "\n",
        "        # special contractions\n",
        "        if word in sc_list:\n",
        "            # split word\n",
        "            word_1 = sc_dic[word][0]\n",
        "            word_2 = sc_dic[word][1]\n",
        "\n",
        "            # replace contraction with two words\n",
        "            i = words_c.index(word)\n",
        "            words_c[i:i+1] = word_1, word_2\n",
        "\n",
        "    return words_c\n",
        "\n",
        "tokenized_top_tweets = [tokenize(tweet) for tweet in top_tweets]\n",
        "\n",
        "for tokenized_tweet in tokenized_top_tweets:\n",
        "    print(tokenized_tweet)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Covid-19', 'Economic', 'Response', ':', 'Cancel', 'Student', 'Loans', 'by', 'Executive', 'Order', '.', '-', 'Sign', 'the', 'Petition', '!', 'https://t.co/BnPXWHv5cr', 'via', '@Change']\n",
            "['Hey', '!', 'The', 'stock', 'markets', 'up', '!', 'Fuck', 'Trump', 'and', 'the', 'Trumpublican', 'Senate', '!', 'https://t.co/4t6mgbaG2C']\n",
            "['@ProjectLincoln', 'blame', 'Trump', 'for', 'no', 'sports', 'because', 'of', 'Covid19', '@MeidasTouch']\n",
            "['Do', 'u', 'guys', 'think', 'that', 'reopening', 'of', 'the', 'dinning', 'in', 'the', 'restaurants', 'of', 'Lahore', 'will', 'bring', 'any', 'change', 'in', 'our', 'lives', '?', 'Nothing', 'is', 'going', 'to', 'change', 'I', 'think', 'cause', 'many', 'people', 'have', 'learned', 'cooking', ',', 'painting', 'and', 'many', 'new', 'stuff', 'during', 'lockdown', 'and', 'I', 'think', 'we', 'all', 'will', 'continue', 'those', 'things', '1/2']\n",
            "['@NBCSAthletics', 'Ya', 'just', 'knew', 'the', 'season', 'would', 'n’t', 'go', 'by', 'without', 'some', 'bench', 'clearing', 'brawls', '.', '.', '.', 'Covid', 'or', 'not', '.', 'Behaviors', 'ca', 'n’t', 'be', 'changed', 'because', 'of', 'rules', ',', 'sadly']\n",
            "['You', '’re', 'comparing', 'apples', 'and', 'oranges', '.', 'The', 'NCAA', 'has', 'a', 'lot', 'more', 'to', 'balance', 'than', 'the', 'NFL', '.', 'https://t.co/oeMsbgeem7']\n",
            "['THANK', 'YOU', '@POTUS', 'Absolutely', 'NO', 'MONEY', '/', 'NO', 'BAILOUTS', 'for', 'Irresponsibly', 'Run', 'Democrat', 'Cities', '.', '.', '.', '#NoBailouts', 'https://t.co/bWLELcCutV']\n",
            "['#Texas', '#USA', '#NorthAmerica', 'Cases', ':', '509,539', '(', '+', '21', ')', 'Death', ':', '8,583', 'Recovered', ':', '344,845', 'Critical', ':', '1,754', 'New', '%', ':', '0.3', '%', 'Death', '%', ':', '1.7', '%', 'Population', '%', ':', '1.8', '%', '#CoronaVirus', '#Covid19', '#SarsCov2', '#Forecast', 'https://t.co/yHbd9gl1uz', 'https://t.co/sRulRFOeUx']\n",
            "['New', 'Zealand', 'celebrated', 'their', '100', 'th', 'day', 'without', 'COVID', 'today', '.', 'A', 'society', 'that', 'works', 'together', 'for', 'the', 'common', 'good', ':', 'that', \"'s\", 'what', 'actual', 'freedom', 'looks', 'like', '.', 'https://t.co/7qppin3QRM']\n",
            "['It', '’s', 'going', 'to', 'be', 'screen', 'time', 'all', 'the', 'time', 'for', 'kindergartners', 'and', 'graduate', 'students', 'alike', '.', 'Teachers', 'are', 'threatening', 'strikes', '.', 'And', 'students', 'are', 'already', 'coming', 'home', 'infected', 'with', 'the', 'coronavirus', ',', 'which', 'has', 'upended', 'American', 'education', '.', 'https://t.co/O3eT06wmrU']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdUUFy_CwTwQ"
      },
      "source": [
        "```\n",
        "['Covid-19', 'Economic', 'Response', ':', 'Cancel', 'Student', 'Loans', 'by', 'Executive', 'Order', '.', '-', 'Sign', 'the', 'Petition', '!', 'https://t.co/BnPXWHv5cr', 'via', '@Change']\n",
        "['Hey', '!', 'The', 'stock', 'markets', 'up', '!', 'Fuck', 'Trump', 'and', 'the', 'Trumpublican', 'Senate', '!', 'https://t.co/4t6mgbaG2C']\n",
        "['@ProjectLincoln', 'blame', 'Trump', 'for', 'no', 'sports', 'because', 'of', 'Covid19', '@MeidasTouch']\n",
        "['Do', 'u', 'guys', 'think', 'that', 'reopening', 'of', 'the', 'dinning', 'in', 'the', 'restaurants', 'of', 'Lahore', 'will', 'bring', 'any', 'change', 'in', 'our', 'lives', '?', 'Nothing', 'is', 'going', 'to', 'change', 'I', 'think', 'cause', 'many', 'people', 'have', 'learned', 'cooking', ',', 'painting', 'and', 'many', 'new', 'stuff', 'during', 'lockdown', 'and', 'I', 'think', 'we', 'all', 'will', 'continue', 'those', 'things', '1/2']\n",
        "['@NBCSAthletics', 'Ya', 'just', 'knew', 'the', 'season', 'would', 'n’t', 'go', 'by', 'without', 'some', 'bench', 'clearing', 'brawls', '...', 'Covid', 'or', 'not', '.', 'Behaviors', 'ca', 'n’t', 'be', 'changed', 'because', 'of', 'rules', ',', 'sadly']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BZCXuVwx7o_"
      },
      "source": [
        "# compare output to nltk\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# tokenized_top_tweets = [nltk.word_tokenize(tweet) for tweet in top_tweets]\n",
        "\n",
        "# for tokenized_tweet in tokenized_top_tweets:\n",
        "    # print(tokenized_tweet)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ciy3qF-DkT9"
      },
      "source": [
        "# test code\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# s = \"hello world\"\n",
        "# print(tokenize(s))\n",
        "# print(nltk.word_tokenize(s))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEmNfrQqKoMY"
      },
      "source": [
        "# Problem 2: Post-process the tokenized tweets\n",
        "\n",
        "Clean the tokenized tweets such that usernames are repalced with `@USER`, URLs are replaced with `URL`, punctuations are removed and words are lower-cased.\n",
        "\n",
        "The output for the top ten tweets after cleaning should look something like this\n",
        "\n",
        "```\n",
        "['covid-19', 'economic', 'response', 'cancel', 'student', 'loans', 'by', 'executive', 'order', 'sign', 'the', 'petition', 'URL', 'via', '@USER']\n",
        "['hey', 'the', 'stock', 'markets', 'up', 'fuck', 'trump', 'and', 'the', 'trumpublican', 'senate', 'URL']\n",
        "['@USER', 'blame', 'trump', 'for', 'no', 'sports', 'because', 'of', 'covid19', '@USER']\n",
        "['do', 'u', 'guys', 'think', 'that', 'reopening', 'of', 'the', 'dinning', 'in', 'the', 'restaurants', 'of', 'lahore', 'will', 'bring', 'any', 'change', 'in', 'our', 'lives', 'nothing', 'is', 'going', 'to', 'change', 'i', 'think', 'cause', 'many', 'people', 'have', 'learned', 'cooking', 'painting', 'and', 'many', 'new', 'stuff', 'during', 'lockdown', 'and', 'i', 'think', 'we', 'all', 'will', 'continue', 'those', 'things', '1/2']\n",
        "['@USER', 'ya', 'just', 'knew', 'the', 'season', 'would', 'n’t', 'go', 'by', 'without', 'some', 'bench', 'clearing', 'brawls', 'covid', 'or', 'not', 'behaviors', 'ca', 'n’t', 'be', 'changed', 'because', 'of', 'rules', 'sadly']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMWFqI9KbDnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7356be43-8958-4358-c98a-5e5ff14ab0d2"
      },
      "source": [
        "punctuation = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\",\n",
        "               \".\", \"/\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\"]\n",
        "\n",
        "# function to clean a tweet\n",
        "def clean_a_tweet(tokenized_tweet):\n",
        "    to_remove = []\n",
        "\n",
        "    clean_tokenized_tweet = copy.deepcopy(tokenized_tweet)\n",
        "\n",
        "    # remove punctuation\n",
        "    for word in clean_tokenized_tweet:\n",
        "        if word in punctuation:\n",
        "            to_remove.append(word)\n",
        "    \n",
        "    for r in to_remove:\n",
        "        clean_tokenized_tweet.remove(r)\n",
        "\n",
        "    for word in clean_tokenized_tweet:\n",
        "        # replace usernames with @USER\n",
        "        if (len(word) > 1) and (word[0] == \"@\"):\n",
        "            i = clean_tokenized_tweet.index(word)\n",
        "            clean_tokenized_tweet[i] = \"@USER\"\n",
        "\n",
        "        # replace URLs with URL\n",
        "        elif (len(word) > 6) and (word[:6] == \"https:\"):\n",
        "            i = clean_tokenized_tweet.index(word)\n",
        "            clean_tokenized_tweet[i] = \"URL\"\n",
        "\n",
        "        # make words lower-case\n",
        "        else:\n",
        "            word_lc = word.lower()\n",
        "            \n",
        "            i = clean_tokenized_tweet.index(word)\n",
        "            clean_tokenized_tweet[i] = word_lc\n",
        "\n",
        "    return clean_tokenized_tweet\n",
        "\n",
        "anonymized_top_tweets = [clean_a_tweet(tokenized_tweet) for tokenized_tweet in tokenized_top_tweets]\n",
        "for tokenized_tweet in anonymized_top_tweets:\n",
        "    print(tokenized_tweet)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['covid-19', 'economic', 'response', 'cancel', 'student', 'loans', 'by', 'executive', 'order', 'sign', 'the', 'petition', 'URL', 'via', '@USER']\n",
            "['hey', 'the', 'stock', 'markets', 'up', 'fuck', 'trump', 'and', 'the', 'trumpublican', 'senate', 'URL']\n",
            "['@USER', 'blame', 'trump', 'for', 'no', 'sports', 'because', 'of', 'covid19', '@USER']\n",
            "['do', 'u', 'guys', 'think', 'that', 'reopening', 'of', 'the', 'dinning', 'in', 'the', 'restaurants', 'of', 'lahore', 'will', 'bring', 'any', 'change', 'in', 'our', 'lives', 'nothing', 'is', 'going', 'to', 'change', 'i', 'think', 'cause', 'many', 'people', 'have', 'learned', 'cooking', 'painting', 'and', 'many', 'new', 'stuff', 'during', 'lockdown', 'and', 'i', 'think', 'we', 'all', 'will', 'continue', 'those', 'things', '1/2']\n",
            "['@USER', 'ya', 'just', 'knew', 'the', 'season', 'would', 'n’t', 'go', 'by', 'without', 'some', 'bench', 'clearing', 'brawls', 'covid', 'or', 'not', 'behaviors', 'ca', 'n’t', 'be', 'changed', 'because', 'of', 'rules', 'sadly']\n",
            "['you', '’re', 'comparing', 'apples', 'and', 'oranges', 'the', 'ncaa', 'has', 'a', 'lot', 'more', 'to', 'balance', 'than', 'the', 'nfl', 'URL']\n",
            "['thank', 'you', '@USER', 'absolutely', 'no', 'money', 'no', 'bailouts', 'for', 'irresponsibly', 'run', 'democrat', 'cities', '#nobailouts', 'URL']\n",
            "['#texas', '#usa', '#northamerica', 'cases', '509,539', '21', 'death', '8,583', 'recovered', '344,845', 'critical', '1,754', 'new', '0.3', 'death', '1.7', 'population', '1.8', '#coronavirus', '#covid19', '#sarscov2', '#forecast', 'URL', 'URL']\n",
            "['new', 'zealand', 'celebrated', 'their', '100', 'th', 'day', 'without', 'covid', 'today', 'a', 'society', 'that', 'works', 'together', 'for', 'the', 'common', 'good', 'that', \"'s\", 'what', 'actual', 'freedom', 'looks', 'like', 'URL']\n",
            "['it', '’s', 'going', 'to', 'be', 'screen', 'time', 'all', 'the', 'time', 'for', 'kindergartners', 'and', 'graduate', 'students', 'alike', 'teachers', 'are', 'threatening', 'strikes', 'and', 'students', 'are', 'already', 'coming', 'home', 'infected', 'with', 'the', 'coronavirus', 'which', 'has', 'upended', 'american', 'education', 'URL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-pSFRY-tMXr"
      },
      "source": [
        "```\n",
        "['covid-19', 'economic', 'response', 'cancel', 'student', 'loans', 'by', 'executive', 'order', 'sign', 'the', 'petition', 'URL', 'via', '@USER']\n",
        "['hey', 'the', 'stock', 'markets', 'up', 'fuck', 'trump', 'and', 'the', 'trumpublican', 'senate', 'URL']\n",
        "['@USER', 'blame', 'trump', 'for', 'no', 'sports', 'because', 'of', 'covid19', '@USER']\n",
        "['do', 'u', 'guys', 'think', 'that', 'reopening', 'of', 'the', 'dinning', 'in', 'the', 'restaurants', 'of', 'lahore', 'will', 'bring', 'any', 'change', 'in', 'our', 'lives', 'nothing', 'is', 'going', 'to', 'change', 'i', 'think', 'cause', 'many', 'people', 'have', 'learned', 'cooking', 'painting', 'and', 'many', 'new', 'stuff', 'during', 'lockdown', 'and', 'i', 'think', 'we', 'all', 'will', 'continue', 'those', 'things', '1/2']\n",
        "['@USER', 'ya', 'just', 'knew', 'the', 'season', 'would', 'n’t', 'go', 'by', 'without', 'some', 'bench', 'clearing', 'brawls', 'covid', 'or', 'not', 'behaviors', 'ca', 'n’t', 'be', 'changed', 'because', 'of', 'rules', 'sadly']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spLPgjMevd4l"
      },
      "source": [
        "# test code\n",
        "# tester = [\"hello\", \"Hello\", \"HELLO\"]\n",
        "# clean_a_tweet(tester)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVkL6n6jU1LU"
      },
      "source": [
        "# Problem 3: Unigrams and Bigrams\n",
        "\n",
        "Read **all** tweets from the file `20000_tweets.txt` and print the top 10 unigrams and bigrams. Consider only the English tweets. You have to use `tokenize` and `clean_a_tweet` function.\n",
        "\n",
        "Output format\n",
        "\n",
        "```\n",
        "Top 10 Unigrams\n",
        "@USER\t16698\n",
        "the\t16666\n",
        "URL\t12610\n",
        "to\t11866\n",
        "a\t9845\n",
        "....\n",
        "....\n",
        "....\n",
        "....\n",
        "....\n",
        "\n",
        "Top 10 Bigrams\n",
        "@USER @USER\t7494\n",
        "in the\t1280\n",
        "of the\t1265\n",
        "it ’s\t860\n",
        "a mask\t793\n",
        "....\n",
        "....\n",
        "....\n",
        "....\n",
        "....\n",
        "```\n",
        "\n",
        "Your numbers need not match the above word frequencies. Depending on your tokenizer, you may get different numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQs6Th7AlTtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc112a51-3b41-43fc-d81a-350ba5fb2a75"
      },
      "source": [
        "# Write your code to build top 10 unigrams and bigrams\n",
        "# Hint: You can use python dictionary or collections.Counter().\n",
        "from collections import Counter\n",
        "\n",
        "# read all tweets from 20000_tweets.txt\n",
        "f = open(\"/content/drive/My Drive/tweets/20000_tweets.txt\", \"r\")\n",
        "\n",
        "all_tweets = []\n",
        "for tweet in f:\n",
        "  all_tweets.append(tweet)\n",
        "\n",
        "f.close()\n",
        "\n",
        "# tokenize all tweets\n",
        "tokenized_all_tweets = [tokenize(tweet) for tweet in all_tweets]\n",
        "\n",
        "# clean all tweets\n",
        "anonymized_all_tweets = [clean_a_tweet(tokenized_tweet) for tokenized_tweet in tokenized_all_tweets]\n",
        "\n",
        "# get unigrams\n",
        "def get_unigrams(list_of_tweets, top_n=10, silent=False):\n",
        "    # one list for all unigrams\n",
        "    all_unigrams = []\n",
        "    for tokenized_tweet in list_of_tweets:\n",
        "        all_unigrams = all_unigrams + tokenized_tweet\n",
        "\n",
        "    unigram_count = Counter(all_unigrams)\n",
        "\n",
        "    if not silent:\n",
        "    # print the top n unigrams\n",
        "        top_n_unigrams = unigram_count.most_common(top_n)\n",
        "        print('Top', top_n, 'Unigrams')\n",
        "        for unigram in top_n_unigrams:\n",
        "            print(unigram[0], '    ', unigram[1])\n",
        "\n",
        "    return dict(unigram_count)\n",
        "\n",
        "unigrams = get_unigrams(anonymized_all_tweets)\n",
        "print()\n",
        "\n",
        "# get bigrams\n",
        "def get_bigrams(list_of_tweets, top_n=10, silent=False):\n",
        "    # one list for all bigrams\n",
        "    all_bigrams = []\n",
        "    for tokenized_tweet in list_of_tweets:\n",
        "        if len(tokenized_tweet) > 1:\n",
        "            for i in range(len(tokenized_tweet)-1):\n",
        "                bigram = tokenized_tweet[i] + \" \" + tokenized_tweet[i+1]\n",
        "                all_bigrams.append(bigram)\n",
        "\n",
        "    bigram_count = Counter(all_bigrams)\n",
        "\n",
        "    if not silent:\n",
        "    # print the top n bigrams\n",
        "        top_n_bigrams = bigram_count.most_common(top_n)\n",
        "        print('Top', top_n, 'Bigrams')\n",
        "        for bigram in top_n_bigrams:\n",
        "            print(bigram[0], '    ', bigram[1])\n",
        "\n",
        "    return dict(bigram_count)\n",
        "\n",
        "bigrams = get_bigrams(anonymized_all_tweets)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Unigrams\n",
            "@USER      16837\n",
            "the      16662\n",
            "URL      12624\n",
            "to      11855\n",
            "a      9851\n",
            "and      8544\n",
            "of      7810\n",
            "i      6576\n",
            "is      6475\n",
            "in      6301\n",
            "\n",
            "Top 10 Bigrams\n",
            "@USER @USER      7532\n",
            "in the      1280\n",
            "of the      1264\n",
            "it ’s      856\n",
            "a mask      790\n",
            "this is      748\n",
            "@USER i      725\n",
            "the pandemic      717\n",
            "wear a      698\n",
            "URL URL      692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqQKOrTyH0NO"
      },
      "source": [
        "```\n",
        "Top 10 Unigrams\n",
        "@USER\t16698\n",
        "the\t16666\n",
        "URL\t12610\n",
        "to\t11866\n",
        "a\t9845\n",
        "...\n",
        "\n",
        "Top 10 Bigrams\n",
        "@USER @USER\t7494\n",
        "in the\t1280\n",
        "of the\t1265\n",
        "it ’s\t860\n",
        "a mask\t793\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vGwr8h5piIC"
      },
      "source": [
        "# test code\n",
        "# list(unigrams)[-25:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q-FpH2k7akG"
      },
      "source": [
        "# Problem 4: How frequent are the following unigrams and bigrams?\n",
        "\n",
        "```\n",
        "covid\n",
        "coronavirus\n",
        "republicans\n",
        "democrats\n",
        "social distancing\n",
        "wear mask\n",
        "no mask\n",
        "donald trump\n",
        "joe biden\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "covid\t4437\n",
        "coronavirus\t1368\n",
        "...\n",
        "...\n",
        "social distancing\t568\n",
        "wear mask\t19\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Jcaf1T9mHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cfe4c7-348c-47b5-e3e4-e63331363a37"
      },
      "source": [
        "# Write your code to print the frequencies of the above unigrams and bigrams\n",
        "# Hint: reuse the dictionaries you built in the previous cell\n",
        "word_list_1 = [\"covid\", \"coronavirus\", \"republicans\", \"democrats\"]\n",
        "word_list_2 = [\"social distancing\", \"wear mask\", \"no mask\", \"donald trump\", \"joe biden\"]\n",
        "\n",
        "for word in word_list_1:\n",
        "  if word in unigrams:\n",
        "    print(f\"{word}    {unigrams[word]}\")\n",
        "\n",
        "for word in word_list_2:\n",
        "  if word in bigrams:\n",
        "    print(f\"{word}    {bigrams[word]}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covid    4344\n",
            "coronavirus    1353\n",
            "republicans    58\n",
            "democrats    160\n",
            "social distancing    558\n",
            "wear mask    19\n",
            "no mask    19\n",
            "donald trump    62\n",
            "joe biden    23\n"
          ]
        }
      ]
    }
  ]
}